# List
- [Assessing the Fairness of AI Systems: AI Practitioners’ Processes, Challenges, and Needs for Support](https://arxiv.org/abs/2112.05675)


# Notes

## Assessing the Fairness of AI Systems: AI Practitioners’ Processes, Challenges, and Needs for Support
This group has a previous checklist on ML fairness. There should be a formal process, such as JIRA tickets, to fit into the prioritization queue.

RQ: 
- what are existing processes
- what support do practitioners need 

Method: semi-structured interviews with PMs, follow up workshop with teams
- Product/Program manager:

During the workshop, researchers share
- good performance/metrics
- use cases
- which direct stakeholders & demographic groups
- what data for disaggregated evaluation
- determination

Results
- mixed results. business metrics different from ml. our features are often judged by usage. "By whom?"
- practical metrics
- stakeholder groups have conflicts on prioritization. 
- B2B context this applies to (internal client)
- pressure to ship across languages and locales

Reflection
- workshop way to collect insights and generate conversation was genius. 
- what do the workshops look like? Is it a team interview? Or is there something facilitative? an education workshop? brown-bag? What's listed on the team calendar?
- Besides the connection/power of PM over the team members, what are the incentives for PM to get these researchers 
- What does the disaggregated evaluation?
- what does the sharing stakeholders really look like? is it a channel in slack outside of the team members who are participating in team meetings?
- gender label. limited data collection, i dont think that's true. third-party data

Depending on integration of ad-tech, demographic data are definitely feasible. First party - pii. i
